#!/bin/bash#SBATCH --job-name=dynamic_array#SBATCH --time=2:59:00#SBATCH --cpus-per-task=2#SBATCH --mem=4000M#SBATCH -D ./# Environment setupmodule load slurm_setupsource "$HOME/miniconda3/etc/profile.d/conda.sh"conda activate steinstadion-envexport TMPDIR=/tmpexport MPLCONFIGDIR=/tmp/matplotlibmkdir -p /tmp/matplotlibexport PYTHONPATH=$PYTHONPATH:$HOME/steinstadion# --- Split the COMMANDS environment variable into an array ---IFS=":::" read -r -a CMD_ARRAY <<< "$COMMANDS"# --- Pick the command corresponding to this array task ---CMD="${CMD_ARRAY[$((SLURM_ARRAY_TASK_ID-1))]}"echo "Running command #${SLURM_ARRAY_TASK_ID}: $CMD"eval "$CMD"# Example usage of passing an array of commands to this Slurm array job:## Define a list of commands in Python:# commands = [#     "python script1.py --arg 1",#     "python script2.py --arg 2",#     "python script3.py --arg 3",#     # ... up to 8000 commands# ]## Convert the list to a single string using a delimiter (here ':::'):# commands_str = ":::".join(commands)## Determine the number of commands to set the Slurm array size:# NUM_COMMANDS=${len(commands)}## Submit the array job to Slurm with a maximum of 200 concurrent jobs:# sbatch --array=1-${NUM_COMMANDS}%200 --export=ALL,COMMANDS="$commands_str" myarray.sh