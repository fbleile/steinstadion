#!/bin/bash#SBATCH --job-name=dynamic_array#SBATCH --time=2:59:00#SBATCH --cpus-per-task=2#SBATCH --mem=4000M#SBATCH -D ./# Debug: print environment variablesecho "HOME = $HOME"echo "USER = $USER"echo "TMPDIR = $TMPDIR"echo "PYTHONPATH = $PYTHONPATH"# Environment setupmodule load slurm_setupsource "$HOME/miniconda3/etc/profile.d/conda.sh"conda activate steinstadion-envexport TMPDIR=/tmpexport MPLCONFIGDIR=/tmp/matplotlibmkdir -p /tmp/matplotlibexport PYTHONPATH=$PYTHONPATH:$HOME/steinstadion# Read the command for this array taskCMD=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "$1")echo "Running command: $CMD"eval "$CMD"# Optional: delete file at the end if you really want# rm -f "$1"# Example usage of passing an array of commands to this Slurm array job:## Define a list of commands in Python:# commands = [#     "python script1.py --arg 1",#     "python script2.py --arg 2",#     "python script3.py --arg 3",#     # ... up to 8000 commands# ]## Convert the list to a single string using a delimiter (here ':::'):# commands_str = ":::".join(commands)## Determine the number of commands to set the Slurm array size:# NUM_COMMANDS=${len(commands)}## Submit the array job to Slurm with a maximum of 200 concurrent jobs:# sbatch --array=1-${NUM_COMMANDS}%200 --export=ALL,COMMANDS="$commands_str" myarray.sh