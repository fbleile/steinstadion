{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f9c99b0-d66d-44f5-9f8c-3108d53fe8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{lcccccc}\n",
      "\\hline\n",
      "Baseline / Dataset & linear-er & linear-sf & scm-er & scm-sf & sergio-er & sergio-sf \\\\\n",
      "\\hline\n",
      "GIES &  / $\\ast$ &  / $\\ast$ &  / $\\ast$ &  /  & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "IGSP & $\\ast$ / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ & \\textcolor{red}{$\\ast$} / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "DCDI & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & \\textcolor{red}{$\\ast$} /  &  /  \\\\\n",
      "LLC & $\\ast$ / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "NODAGS & $\\ast$ / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ &  / $\\ast$ & \\textcolor{red}{$\\ast$} / $\\ast$ &  /  \\\\\n",
      "KDS (Linear) &  /  &  /  & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "KDS (MLP) & \\textcolor{red}{$\\ast$} /  & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ &  /  &  /  &  /  \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Significance tests for \\textbf{SKDS (Linear)}. Black star indicates our method significantly outperforms the baseline; red star indicates baseline significantly outperforms ours. Each cell shows MSE / Wasserstein results.}\n",
      "\\end{table}\n",
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{lcccccc}\n",
      "\\hline\n",
      "Baseline / Dataset & linear-er & linear-sf & scm-er & scm-sf & sergio-er & sergio-sf \\\\\n",
      "\\hline\n",
      "GIES & \\textcolor{red}{$\\ast$} /  &  / $\\ast$ & \\textcolor{red}{$\\ast$} /  & \\textcolor{red}{$\\ast$} / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "IGSP & \\textcolor{red}{$\\ast$} / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "DCDI &  / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ &  / $\\ast$ &  / $\\ast$ \\\\\n",
      "LLC & \\textcolor{red}{$\\ast$} / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "NODAGS & \\textcolor{red}{$\\ast$} / $\\ast$ &  / $\\ast$ & $\\ast$ / $\\ast$ &  / $\\ast$ &  / $\\ast$ &  / $\\ast$ \\\\\n",
      "KDS (Linear) & \\textcolor{red}{$\\ast$} / \\textcolor{red}{$\\ast$} & $\\ast$ /  & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ & $\\ast$ / $\\ast$ \\\\\n",
      "KDS (MLP) & \\textcolor{red}{$\\ast$} / \\textcolor{red}{$\\ast$} &  /  &  /  &  /  & $\\ast$ /  &  /  \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Significance tests for \\textbf{SKDS (MLP)}. Black star indicates our method significantly outperforms the baseline; red star indicates baseline significantly outperforms ours. Each cell shows MSE / Wasserstein results.}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Significance Tests for Experimental Results\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# ------------------------------\n",
    "# Paths and dataset configuration\n",
    "# ------------------------------\n",
    "\n",
    "# Base directory containing results CSVs\n",
    "base_dir = os.path.join(\"..\", \"results\")\n",
    "\n",
    "# List of dataset types to load (adjust to your project)\n",
    "from definitions import DEFAULT_DATA_GEN_TYPES\n",
    "\n",
    "# Metrics to test\n",
    "metrics_to_plot = [\"mse_test\", \"wasser_test\"]\n",
    "\n",
    "# Methods configuration (adjust according to your METHODS_CONFIG)\n",
    "from experiment.plot_config import METHODS_CONFIG\n",
    "METHODS_CONFIG = [m for m in METHODS_CONFIG if m[0] != '__true__']\n",
    "method_display = {m[0]: m[3] for m in METHODS_CONFIG if m[0] is not True}\n",
    "\n",
    "# Define \"our methods\" and baselines\n",
    "our_methods = [\"ours-linear_u_diag\", \"ours-lnl_u_diag\"]\n",
    "all_methods = list(method_display.keys())\n",
    "baselines = [m for m in all_methods if m not in our_methods]\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "# Practical improvement level\n",
    "beta = 0.05\n",
    "\n",
    "# ------------------------------\n",
    "# Load CSVs\n",
    "# ------------------------------\n",
    "all_data = {}\n",
    "for data_type in DEFAULT_DATA_GEN_TYPES:\n",
    "    csv_path = os.path.join(base_dir, f\"{data_type}/summary_00_01_00/mean\",\n",
    "                            f\"df-{data_type}-summary_00_01_00.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    all_data[data_type] = df\n",
    "\n",
    "def median_log_ratio_ci(our, base, n_boot=10000, alpha=0.05):\n",
    "    our = np.maximum(our, 1e-12)\n",
    "    base = np.maximum(base, 1e-12)\n",
    "    log_ratio = np.log(our) - np.log(base)\n",
    "    boot_stats = []\n",
    "    n = len(log_ratio)\n",
    "    for _ in range(n_boot):\n",
    "        idx = np.random.randint(0, n, size=n)\n",
    "        boot_stats.append(np.median(log_ratio[idx]))\n",
    "    lo = np.percentile(boot_stats, 100 * (alpha/2))\n",
    "    hi = np.percentile(boot_stats, 100 * (1 - alpha/2))\n",
    "    return lo, hi  # in log-space; exp gives multiplicative CI\n",
    "# ------------------------------\n",
    "# Initialize results dictionary\n",
    "# ------------------------------\n",
    "# Structure: results[our_method][baseline_method][data_type] = outcome string\n",
    "results = {our: {base: {} for base in baselines} for our in our_methods}\n",
    "\n",
    "# ------------------------------\n",
    "# Run paired Wilcoxon tests and record detailed outcomes\n",
    "# ------------------------------\n",
    "for data_type, df in all_data.items():\n",
    "    for metric in metrics_to_plot:\n",
    "        df_metric = df[df[\"metric\"] == metric]\n",
    "\n",
    "        for our_method in our_methods:\n",
    "            our_df = df_metric[df_metric[\"method\"] == our_method]\n",
    "\n",
    "            for baseline in baselines:\n",
    "                baseline_df = df_metric[df_metric[\"method\"] == baseline]\n",
    "\n",
    "                # Merge to ensure paired comparison on data_idx and env_idx\n",
    "                paired = pd.merge(\n",
    "                    our_df, baseline_df,\n",
    "                    on=[\"data_idx\", \"env_idx\"],\n",
    "                    suffixes=(\"_our\", \"_base\")\n",
    "                )\n",
    "\n",
    "                # Include NaNs as worst-case\n",
    "                our_vals = paired[\"val_our\"].fillna(np.inf).values\n",
    "                baseline_vals = paired[\"val_base\"].fillna(np.inf).values\n",
    "\n",
    "                # Skip if no pairs\n",
    "                if len(our_vals) == 0:\n",
    "                    continue\n",
    "\n",
    "                # assume paired arrays our_vals and baseline_vals (positive)\n",
    "                eps = 1e-12  # to guard against zeros\n",
    "                our = np.maximum(our_vals, eps)\n",
    "                base = np.maximum(baseline_vals, eps)\n",
    "                \n",
    "                log_ratio = np.log(our) - np.log(base)   # r_i\n",
    "                threshold = np.log(1 - beta)                 # log(1 - beta)\n",
    "                \n",
    "                # test H1: median(log_ratio) < threshold  <=> median(log_ratio - threshold) < 0\n",
    "                d_1 = log_ratio - threshold\n",
    "                d_2 = - log_ratio - threshold\n",
    "                \n",
    "                # wilcoxon of d against 0: call wilcoxon(d, np.zeros_like(d), alternative=\"less\")\n",
    "                stat_1, p_value_1 = wilcoxon(d_1, np.zeros_like(d_1), alternative=\"less\")\n",
    "                stat_2, p_value_2 = wilcoxon(d_2, np.zeros_like(d_2), alternative=\"less\")\n",
    "\n",
    "                # Initialize entry if not exists\n",
    "                if data_type not in results[our_method][baseline]:\n",
    "                    results[our_method][baseline][data_type] = [\n",
    "                        {'ours_better': False, 'baseline_better': False},\n",
    "                        {'ours_better': False, 'baseline_better': False}\n",
    "                    ]  # [mse, wasser]\n",
    "\n",
    "                # Assign outcome based on metric\n",
    "                metric_idx = 0 if metric == \"mse_test\" else 1\n",
    "                if p_value_1 < alpha:\n",
    "                    results[our_method][baseline][data_type][metric_idx]['ours_better'] = True\n",
    "                if p_value_2 < alpha:\n",
    "                    results[our_method][baseline][data_type][metric_idx]['baseline_better'] = True\n",
    "\n",
    "\n",
    "                # lo, hi = median_log_ratio_ci(our_vals, baseline_vals)\n",
    "                # print(\"median ratio CI:\", np.exp(lo), np.exp(hi))\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Generate LaTeX tables: one per \"our method\" with red/black stars\n",
    "# ------------------------------\n",
    "from IPython.display import display, Latex\n",
    "from sympy import preview\n",
    "\n",
    "for our_method in our_methods:\n",
    "    latex_lines = [\n",
    "        \"\\\\begin{table}[h]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\begin{{tabular}}{{l{'c' * len(DEFAULT_DATA_GEN_TYPES)}}}\",\n",
    "        \"\\\\hline\",\n",
    "        \"Baseline / Dataset & \" + \" & \".join(DEFAULT_DATA_GEN_TYPES) + \" \\\\\\\\\",\n",
    "        \"\\\\hline\"\n",
    "    ]\n",
    "\n",
    "    for baseline in baselines:\n",
    "        row = [method_display.get(baseline, baseline)]\n",
    "        for data_type in DEFAULT_DATA_GEN_TYPES:\n",
    "            cell = \"\"\n",
    "            if data_type in results[our_method][baseline]:\n",
    "                for metric_idx in range(2):  # 0 = MSE, 1 = Wasser\n",
    "                    entry = results[our_method][baseline][data_type][metric_idx]\n",
    "                    # entry is now a dict with keys 'ours_better' and 'baseline_better'\n",
    "                    symbol = \"\"\n",
    "                    if entry.get('ours_better', False):\n",
    "                        symbol += \"$\\\\ast$\"  # black star\n",
    "                    if entry.get('baseline_better', False):\n",
    "                        symbol += \"\\\\textcolor{red}{$\\\\ast$}\"  # red star\n",
    "                    cell += symbol.strip()\n",
    "                    if metric_idx == 0:\n",
    "                        cell += \" / \"  # separate metrics in the cell visually\n",
    "            else:\n",
    "                cell = \" / \"  # no test run\n",
    "            row.append(cell)\n",
    "        latex_lines.append(\" & \".join(row) + \" \\\\\\\\\")\n",
    "\n",
    "    latex_lines.extend([\n",
    "        \"\\\\hline\",\n",
    "        \"\\\\end{tabular}\",\n",
    "        f\"\\\\caption{{Significance tests for {method_display.get(our_method, our_method)}. \"\n",
    "        \"Black star indicates our method significantly outperforms the baseline; \"\n",
    "        \"red star indicates baseline significantly outperforms ours. \"\n",
    "        \"Each cell shows MSE / Wasserstein results.}\",\n",
    "        \"\\\\end{table}\"\n",
    "    ])\n",
    "\n",
    "    latex_table = \"\\n\".join(latex_lines)\n",
    "\n",
    "    # Save table to file\n",
    "    filename = f\"significance_{our_method}.tex\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "    print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253e3fd-c63d-480f-a85c-be68b273844b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
